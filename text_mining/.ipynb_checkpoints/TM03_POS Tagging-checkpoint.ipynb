{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install NLTK POS Tagger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T13:42:34.332277Z",
     "start_time": "2019-06-15T13:42:34.329188Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T13:42:26.846085Z",
     "start_time": "2019-06-15T13:42:26.616849Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d98b417a7b11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The dog eats the big hotdog.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"The dog eats the big hotdog.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(nltk.pos_tag(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('book', 'NN'), ('is', 'VBZ'), ('written', 'VBN'), ('by', 'IN'), ('my', 'PRP$'), ('father', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(word_tokenize(\"The book is written by my father.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('father', 'NN'), ('has', 'VBZ'), ('written', 'VBN'), ('more', 'JJR'), ('than', 'IN'), ('ten', 'JJ'), ('books', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(word_tokenize(\"My father has written more than ten books.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full list of the Penn POS tags\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform POS tagging for all tokens in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 75346\n"
     ]
    }
   ],
   "source": [
    "with open(\"../text_mining_2/corpus.txt\", encoding=\"utf8\") as fin:\n",
    "    text = fin.read()\n",
    "print(\"Number of characters: %d\" % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "tokens_with_tag = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most frequent nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('class', 104), ('bourgeoisie', 89), ('society', 72), ('bourgeois', 69), ('proletariat', 62), ('property', 55), ('production', 52), ('labor', 30), ('existence', 30), ('development', 28), ('industry', 27), ('capital', 22), ('form', 21), ('movement', 19), ('struggle', 17), ('character', 17), ('country', 15), ('abolition', 15), ('time', 14), ('man', 14)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag == 'NN':\n",
    "        noun_counts[word] += 1\n",
    "        \n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can always convert words into lower case, excepting proper nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('class', 104), ('bourgeoisie', 89), ('society', 73), ('bourgeois', 69), ('proletariat', 62), ('property', 56), ('production', 52), ('labor', 30), ('existence', 30), ('development', 28), ('industry', 27), ('capital', 24), ('form', 21), ('abolition', 20), ('movement', 19), ('struggle', 17), ('character', 17), ('country', 15), ('time', 14), ('man', 14)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag == 'NN':\n",
    "        noun_counts[word.lower()] += 1\n",
    "        \n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore other kinds of part of speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('be', 41), ('have', 8), ('do', 8), ('introduce', 5), ('increase', 5), ('lose', 5), ('thus', 4), ('attain', 4), ('let', 4), ('bring', 3), ('become', 3), ('use', 3), ('form', 3), ('abolish', 3), ('acquire', 3), ('vanish', 3), ('take', 3), ('pass', 2), ('work', 2), ('say', 2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag == 'VB':\n",
    "        noun_counts[word.lower()] += 1\n",
    "        \n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Communists', 23), ('Socialism', 21), ('Germany', 13), ('Communism', 12), ('France', 12), ('State', 11), ('England', 9), ('Communist', 7), ('Socialist', 6), ('America', 5), ('_i.e._', 5), ('AND', 4), ('THE', 3), ('II', 3), ('I.', 3), ('Modern', 3), ('Hence', 3), ('Communistic', 3), ('Bourgeois', 3), ('SOCIALISM', 3)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag == 'NNP' or tag == 'NNPS':\n",
    "        noun_counts[word] += 1\n",
    "        \n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', 138), ('has', 68), ('are', 59), ('have', 45), ('be', 41), ('was', 29), ('been', 18), ('do', 18), ('existing', 15), ('were', 14), ('had', 12), ('being', 11), ('working', 10), ('become', 9), ('does', 9), ('made', 8), ('developed', 7), ('see', 7), ('becomes', 7), ('created', 7)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag[0] == 'V':\n",
    "        noun_counts[word.lower()] += 1\n",
    "        \n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With lemmatization for better handle different forms of verbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load WordNet Lemmatizer provided by NTLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('be', 310), ('have', 128), ('do', 31), ('become', 21), ('exist', 20), ('take', 17), ('work', 14), ('develop', 13), ('create', 13), ('make', 13), ('see', 12), ('find', 11), ('lose', 10), ('give', 9), ('increase', 9), ('destroy', 9), ('go', 9), ('abolish', 9), ('carry', 8), ('compel', 8)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag[0] == 'V':\n",
    "        noun_counts[wordnet_lemmatizer.lemmatize(word.lower(), 'v')] += 1\n",
    "        # ADJ (a), ADJ_SAT (s), ADV (r), NOUN (n) or VERB (v)\n",
    "\n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('class', 124), ('bourgeoisie', 91), ('society', 76), ('bourgeois', 75), ('proletariat', 64), ('condition', 59), ('property', 56), ('production', 53), ('industry', 35), ('communist', 34), ('relation', 32), ('mean', 30), ('labor', 30), ('existence', 30), ('form', 28), ('development', 28), ('country', 28), ('socialism', 28), ('capital', 24), ('state', 24)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag[0] == 'N':\n",
    "        noun_counts[wordnet_lemmatizer.lemmatize(word.lower(), 'n')] += 1\n",
    "        # ADJ (a), ADJ_SAT (s), ADV (r), NOUN (n) or VERB (v)\n",
    "\n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('not', 55), ('more', 32), ('only', 29), ('so', 27), ('up', 25), ('therefore', 21), ('most', 17), ('away', 13), ('also', 13), ('longer', 12), ('then', 11), ('no', 10), ('even', 10), ('now', 9), ('everywhere', 9), ('generally', 9), ('ever', 9), ('just', 9), ('out', 9), ('already', 8)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag[0] == 'R':\n",
    "        noun_counts[word.lower()] += 1\n",
    "        # ADJ (a), ADJ_SAT (s), ADV (r), NOUN (n) or VERB (v)\n",
    "\n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('not', 55), ('more', 32), ('only', 29), ('so', 27), ('up', 25), ('therefore', 21), ('most', 17), ('away', 13), ('also', 13), ('longer', 12), ('far', 11), ('then', 11), ('no', 10), ('even', 10), ('now', 9), ('everywhere', 9), ('generally', 9), ('ever', 9), ('just', 9), ('out', 9)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag[0] == 'R':\n",
    "        noun_counts[wordnet_lemmatizer.lemmatize(word.lower(), 'r')] += 1\n",
    "        # ADJ (a), ADJ_SAT (s), ADV (r), NOUN (n) or VERB (v)\n",
    "\n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining Specific Distant Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\tof\t2\t279\n",
      "of\tthe\t1\t242\n",
      "the\tthe\t3\t154\n",
      ",\tthe\t2\t118\n",
      ",\tand\t1\t111\n",
      ",\t,\t2\t109\n",
      "the\tof\t3\t105\n",
      "the\t,\t5\t101\n",
      ".\tThe\t1\t100\n",
      ",\t,\t7\t98\n",
      ",\t,\t4\t97\n",
      "of\t,\t3\t97\n",
      "the\tthe\t7\t95\n",
      "the\t,\t8\t94\n",
      "the\tthe\t6\t92\n",
      ",\t,\t6\t92\n",
      "the\tthe\t4\t92\n",
      ",\tthe\t4\t92\n",
      "of\t,\t2\t91\n",
      "the\t,\t2\t91\n"
     ]
    }
   ],
   "source": [
    "window_size = 9\n",
    "\n",
    "word_pair_counts = Counter()\n",
    "word_pair_distance_counts = Counter()\n",
    "for i in range(len(tokens) - 1):\n",
    "    for distance in range(1, window_size):\n",
    "        if i + distance < len(tokens):\n",
    "            w1 = tokens[i]\n",
    "            w2 = tokens[i + distance]\n",
    "            word_pair_distance_counts[(w1, w2, distance)] += 1\n",
    "            word_pair_counts[(w1, w2)] += 1\n",
    "\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\\t%d\" % (w1, w2, distance, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\tclass\t1\t10\n",
      "be\tclass\t4\t6\n",
      "exist\tsociety\t1\t5\n",
      "be\tclass\t6\t5\n",
      "be\tclass\t7\t4\n",
      "rule\tclass\t1\t4\n",
      "be\tclass\t3\t4\n",
      "be\tsociety\t5\t4\n",
      "work\tparty\t2\t4\n",
      "be\tstruggle\t5\t3\n",
      "have\tsociety\t6\t3\n",
      "pave\tway\t2\t3\n",
      "put\tend\t2\t3\n",
      "be\tbourgeoisie\t8\t3\n",
      "lose\tcharacter\t3\t3\n",
      "be\tbourgeois\t5\t3\n",
      "be\thand\t4\t3\n",
      "be\tcondition\t6\t3\n",
      "fight\tbourgeoisie\t3\t3\n",
      "have\tnothing\t1\t3\n"
     ]
    }
   ],
   "source": [
    "window_size = 9\n",
    "\n",
    "word_pair_counts = Counter()\n",
    "word_pair_distance_counts = Counter()\n",
    "for i in range(len(tokens_with_tag) - 1):\n",
    "    w1, t1 = tokens_with_tag[i]\n",
    "    if t1[0] != 'V':\n",
    "        continue\n",
    "    w1 = wordnet_lemmatizer.lemmatize(w1.lower(), 'v')\n",
    "        \n",
    "    for distance in range(1, window_size):\n",
    "        if i + distance < len(tokens_with_tag):\n",
    "            w2, t2 = tokens_with_tag[i + distance]\n",
    "            if t2[0] == 'N':\n",
    "                w2 = wordnet_lemmatizer.lemmatize(w2.lower(), 'n')\n",
    "                word_pair_distance_counts[(w1, w2, distance)] += 1\n",
    "                word_pair_counts[(w1, w2)] += 1\n",
    "\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\\t%d\" % (w1, w2, distance, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean distance of each verb-noun pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_mean_distances = Counter()\n",
    "\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "    if word_pair_counts[(w1, w2)] > 1:\n",
    "        pair_mean_distances[(w1, w2)] += distance * (c / word_pair_counts[(w1, w2)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the longest, middle, and shortest pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\tcase\t8.000000\t2\n",
      "introduce\tbourgeoisie\t8.000000\t2\n",
      "be\tcommunism\t7.500000\t2\n",
      "have\tcommunism\t7.500000\t2\n",
      "do\tpopulation\t7.500000\t2\n",
      "have\tforce\t7.500000\t2\n",
      "have\tmaster\t7.500000\t2\n",
      "have\truling\t7.500000\t2\n",
      "be\tinterest\t7.500000\t2\n",
      "be\torder\t7.500000\t2\n",
      "be\tmoment\t7.500000\t2\n",
      "have\tman\t7.000000\t2\n",
      "have\tindustry\t7.000000\t2\n",
      "see\tproletariat\t7.000000\t2\n",
      "have\tdissolution\t7.000000\t2\n",
      "be\trelation\t7.000000\t3\n",
      "be\taction\t7.000000\t2\n",
      "be\tmodern\t7.000000\t2\n",
      "leave\tman\t7.000000\t2\n",
      "see\tproduction\t7.000000\t2\n"
     ]
    }
   ],
   "source": [
    "for (w1, w2), distance in pair_mean_distances.most_common(20):\n",
    "    print(\"%s\\t%s\\t%f\\t%d\" % (w1, w2, distance, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\tmean\t5.333333\t6\n",
      "have\tfeudal\t5.333333\t3\n",
      "have\tcharacter\t5.333333\t3\n",
      "be\tlaborer\t5.333333\t3\n",
      "be\tproperty\t5.333333\t9\n",
      "be\tcapitalist\t5.333333\t3\n",
      "be\tclass\t5.238095\t21\n",
      "be\tcapital\t5.166667\t6\n",
      "be\tstruggle\t5.166667\t6\n",
      "be\tcondition\t5.100000\t10\n",
      "be\thand\t5.000000\t4\n",
      "have\tpart\t5.000000\t2\n",
      "compel\tproletariat\t5.000000\t2\n",
      "be\ttime\t5.000000\t5\n",
      "do\tproperty\t5.000000\t5\n",
      "be\tadvance\t5.000000\t2\n",
      "have\tpopulation\t5.000000\t3\n",
      "have\tdependent\t5.000000\t2\n",
      "be\tway\t5.000000\t3\n",
      "be\tslave\t5.000000\t2\n",
      "see\tantagonism\t5.000000\t2\n",
      "be\tbare\t5.000000\t2\n",
      "be\tidea\t5.000000\t4\n",
      "abolish\tproperty\t5.000000\t3\n",
      "create\tproperty\t5.000000\t2\n",
      "convert\tproperty\t5.000000\t2\n",
      "keep\tlaborer\t5.000000\t2\n",
      "increase\tlabor\t5.000000\t2\n",
      "be\tfamily\t5.000000\t2\n",
      "exist\tbourgeoisie\t5.000000\t2\n",
      "replace\teducation\t5.000000\t2\n",
      "do\thistory\t5.000000\t2\n",
      "deaden\tclass\t5.000000\t2\n",
      "be\tman\t4.750000\t4\n",
      "have\tmean\t4.750000\t4\n",
      "be\tcharacter\t4.750000\t4\n",
      "have\thand\t4.600000\t5\n",
      "be\tparty\t4.500000\t2\n",
      "create\tforce\t4.500000\t2\n",
      "abolish\tappropriation\t4.500000\t2\n"
     ]
    }
   ],
   "source": [
    "num_pairs = len(pair_mean_distances)\n",
    "mid = num_pairs // 2\n",
    "for (w1, w2), distance in pair_mean_distances.most_common()[mid-20:mid+20]:\n",
    "    print(\"%s\\t%s\\t%f\\t%d\" % (w1, w2, distance, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\tasunder\t2.000000\t2\n",
      "supply\tproletariat\t2.000000\t2\n",
      "be\tname\t2.000000\t2\n",
      "have\tmeaning\t2.000000\t2\n",
      "keep\tpace\t2.000000\t2\n",
      "stand\tface\t2.000000\t2\n",
      "continue\texistence\t2.000000\t2\n",
      "exist\tproperty\t2.000000\t2\n",
      "go\thand\t2.000000\t2\n",
      "appropriate\tproduct\t1.600000\t5\n",
      "take\tplace\t1.500000\t2\n",
      "increase\tcapital\t1.500000\t2\n",
      "have\tindividuality\t1.500000\t2\n",
      "create\tcondition\t1.500000\t2\n",
      "exist\tsociety\t1.333333\t6\n",
      "rule\tclass\t1.000000\t4\n",
      "rise\tbourgeoisie\t1.000000\t2\n",
      "bourgeois\tsociety\t1.000000\t2\n",
      "introduce\tcommunity\t1.000000\t2\n",
      "lose\tsight\t1.000000\t2\n"
     ]
    }
   ],
   "source": [
    "for (w1, w2), distance in pair_mean_distances.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%d\" % (w1, w2, distance, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out the meaningful verb/noun pairs with deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compel\tproletariat\t5.000000\t0.000000\t2\n",
      "see\tproletariat\t7.000000\t0.000000\t2\n",
      "supply\tproletariat\t2.000000\t0.000000\t2\n",
      "have\tdissolution\t7.000000\t0.000000\t2\n",
      "be\tname\t2.000000\t0.000000\t2\n",
      "be\tantagonism\t4.000000\t0.000000\t2\n",
      "base\tantagonism\t3.000000\t0.000000\t2\n",
      "transform\tproperty\t3.000000\t0.000000\t2\n",
      "dominate\tsociety\t6.000000\t0.000000\t2\n",
      "have\tmeaning\t2.000000\t0.000000\t2\n",
      "determine\tcondition\t4.000000\t0.000000\t2\n",
      "admit\tcase\t3.000000\t0.000000\t2\n",
      "be\tcase\t8.000000\t0.000000\t2\n",
      "introduce\tcommunity\t1.000000\t0.000000\t2\n",
      "introduce\twoman\t3.000000\t0.000000\t2\n",
      "introduce\tbourgeoisie\t8.000000\t0.000000\t2\n",
      "keep\tpace\t2.000000\t0.000000\t2\n",
      "organize\tclass\t4.000000\t0.000000\t2\n",
      "lose\tsight\t1.000000\t0.000000\t2\n",
      "be\taction\t7.000000\t0.000000\t2\n"
     ]
    }
   ],
   "source": [
    "pair_deviations = Counter()\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "    if word_pair_counts[(w1, w2)] > 1:\n",
    "        pair_deviations[(w1, w2)] += c * ((distance - pair_mean_distances[(w1, w2)]) ** 2)\n",
    "    \n",
    "for (w1, w2), dev_tmp in pair_deviations.most_common():\n",
    "    s_2 = dev_tmp / (word_pair_counts[(w1, w2)] - 1)\n",
    "    pair_deviations[(w1, w2)] = s_2 ** 0.5\n",
    "    \n",
    "for (w1, w2), dev in pair_deviations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pave\tway\t2.000000\t0.000000\t3\n",
      "put\tend\t2.000000\t0.000000\t3\n",
      "lose\tcharacter\t3.000000\t0.000000\t3\n",
      "rise\tbourgeoisie\t1.000000\t0.000000\t2\n",
      "get\thand\t3.000000\t0.000000\t2\n",
      "bourgeois\tsociety\t1.000000\t0.000000\t2\n",
      "compel\tproletariat\t5.000000\t0.000000\t2\n",
      "see\tproletariat\t7.000000\t0.000000\t2\n",
      "supply\tproletariat\t2.000000\t0.000000\t2\n",
      "base\tantagonism\t3.000000\t0.000000\t2\n",
      "transform\tproperty\t3.000000\t0.000000\t2\n",
      "dominate\tsociety\t6.000000\t0.000000\t2\n",
      "determine\tcondition\t4.000000\t0.000000\t2\n",
      "admit\tcase\t3.000000\t0.000000\t2\n",
      "introduce\tcommunity\t1.000000\t0.000000\t2\n",
      "introduce\twoman\t3.000000\t0.000000\t2\n",
      "introduce\tbourgeoisie\t8.000000\t0.000000\t2\n",
      "keep\tpace\t2.000000\t0.000000\t2\n",
      "organize\tclass\t4.000000\t0.000000\t2\n",
      "lose\tsight\t1.000000\t0.000000\t2\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "pair_deviations = Counter()\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "    if w1 in stopword_list:\n",
    "        continue\n",
    "    if word_pair_counts[(w1, w2)] > 1:\n",
    "        pair_deviations[(w1, w2)] += c * ((distance - pair_mean_distances[(w1, w2)]) ** 2)\n",
    "    \n",
    "for (w1, w2), dev_tmp in pair_deviations.most_common():\n",
    "    s_2 = dev_tmp / (word_pair_counts[(w1, w2)] - 1)\n",
    "    pair_deviations[(w1, w2)] = s_2 ** 0.5\n",
    "    \n",
    "for (w1, w2), dev in pair_deviations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further filter out the low frequent pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find\twork\t2.666667\t2.886751\t3\n",
      "abolish\tproperty\t5.000000\t2.645751\t3\n",
      "join\tclass\t5.666667\t2.516611\t3\n",
      "fight\tbourgeoisie\t3.800000\t2.387467\t5\n",
      "work\tclass\t2.000000\t2.374103\t12\n",
      "represent\tinterest\t3.666667\t2.081666\t3\n",
      "mean\tbourgeois\t4.333333\t2.081666\t3\n",
      "exist\tthing\t4.000000\t1.732051\t3\n",
      "revolutionize\tproduction\t3.333333\t1.154701\t3\n",
      "intend\tproperty\t6.666667\t1.154701\t3\n",
      "concentrate\thand\t3.666667\t1.154701\t3\n",
      "attain\tend\t3.000000\t1.000000\t3\n",
      "exist\tsociety\t1.333333\t0.816497\t6\n",
      "produce\tproduct\t3.500000\t0.577350\t4\n",
      "appropriate\tproduct\t1.600000\t0.547723\t5\n",
      "rule\tclass\t1.000000\t0.000000\t4\n",
      "work\tparty\t2.000000\t0.000000\t4\n",
      "pave\tway\t2.000000\t0.000000\t3\n",
      "put\tend\t2.000000\t0.000000\t3\n",
      "lose\tcharacter\t3.000000\t0.000000\t3\n"
     ]
    }
   ],
   "source": [
    "pair_deviations = Counter()\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "    if w1 in stopword_list:\n",
    "        continue\n",
    "    if word_pair_counts[(w1, w2)] > 2:\n",
    "        pair_deviations[(w1, w2)] += c * ((distance - pair_mean_distances[(w1, w2)]) ** 2)\n",
    "    \n",
    "for (w1, w2), dev_tmp in pair_deviations.most_common():\n",
    "    s_2 = dev_tmp / (word_pair_counts[(w1, w2)] - 1)\n",
    "    pair_deviations[(w1, w2)] = s_2 ** 0.5\n",
    "    \n",
    "for (w1, w2), dev in pair_deviations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General method for distant collocation mining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A handy lemmatizer \n",
    "# WordNet Style: ADJ (a), ADJ_SAT (s), ADV (r), NOUN (n) or VERB (v)\n",
    "# Penn Style: ADJ (J*), ADJ_SAT (J*), ADV (R*), NOUN (N*), or VERB (V*) \n",
    "def lemmatize_verbose(word, pos):\n",
    "    if pos[0] == 'J':\n",
    "        return wordnet_lemmatizer.lemmatize(word, 'a')\n",
    "    elif pos[0] == 'R':\n",
    "        return wordnet_lemmatizer.lemmatize(word, 'r')\n",
    "    elif pos[0] == 'N':\n",
    "        return wordnet_lemmatizer.lemmatize(word, 'n')\n",
    "    elif pos[0] == 'V':\n",
    "        return wordnet_lemmatizer.lemmatize(word, 'v')\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "\n",
    "def lemmatize_shorter(word, pos):\n",
    "    if pos[0] == 'J':\n",
    "        pos = 'a'\n",
    "    elif pos[0] == 'R':\n",
    "        pos = 'r'\n",
    "    elif pos[0] == 'N':\n",
    "        pos = 'n'\n",
    "    elif pos[0] == 'V':\n",
    "        pos = 'v'\n",
    "    else:\n",
    "        return word\n",
    "    return wordnet_lemmatizer.lemmatize(word, pos)\n",
    "\n",
    "\n",
    "def lemmatize_smarter(word, pos):\n",
    "    if pos[0] in ['R', 'N', 'V']:\n",
    "        pos = pos[0].lower()\n",
    "    elif pos[0] == 'J':\n",
    "        pos = 'a'\n",
    "    else:\n",
    "        return word\n",
    "    return wordnet_lemmatizer.lemmatize(word, pos)\n",
    "\n",
    "\n",
    "# Recommended implementation.\n",
    "def lemmatize(word, pos):\n",
    "    mapping = {'J': 'a', 'R': 'r', 'N': 'n', 'V': 'v'}\n",
    "    if pos[0] in mapping:\n",
    "        return wordnet_lemmatizer.lemmatize(word, mapping[pos[0]])\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count all pairs.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distant_collocations(tokens_with_tag, pos1, pos2, min_cut=2, window_size=9):\n",
    "    word_pair_counts = Counter()\n",
    "    word_pair_distance_counts = Counter()\n",
    "    for i in range(len(tokens_with_tag) - 1):\n",
    "        w1, t1 = tokens_with_tag[i]\n",
    "        if not t1.startswith(pos1):\n",
    "            continue\n",
    "        w1 = lemmatize(w1.lower(), t1)\n",
    "        for distance in range(1, window_size):\n",
    "            if i + distance < len(tokens_with_tag):\n",
    "                w2, t2 = tokens_with_tag[i + distance]\n",
    "                if t2.startswith(pos2):\n",
    "                    w2 = lemmatize(w2.lower(), t2)\n",
    "                    word_pair_distance_counts[(w1, w2, distance)] += 1\n",
    "                    word_pair_counts[(w1, w2)] += 1\n",
    "    \n",
    "    pair_mean_distances = Counter()\n",
    "\n",
    "    for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "        if word_pair_counts[(w1, w2)] > 1:\n",
    "            pair_mean_distances[(w1, w2)] += distance * (c / word_pair_counts[(w1, w2)])\n",
    "\n",
    "    pair_deviations = Counter()\n",
    "    for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "        if w1 in stopword_list:\n",
    "            continue\n",
    "        if word_pair_counts[(w1, w2)] > min_cut:\n",
    "            pair_deviations[(w1, w2)] += c * ((distance - pair_mean_distances[(w1, w2)]) ** 2)\n",
    "    \n",
    "    for (w1, w2), dev_tmp in pair_deviations.most_common():\n",
    "        s_2 = dev_tmp / (word_pair_counts[(w1, w2)] - 1)\n",
    "        pair_deviations[(w1, w2)] = s_2 ** 0.5\n",
    "    \n",
    "    return pair_deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find\twork\t2.666667\t2.886751\t3\n",
      "abolish\tproperty\t5.000000\t2.645751\t3\n",
      "join\tclass\t5.666667\t2.516611\t3\n",
      "fight\tbourgeoisie\t3.800000\t2.387467\t5\n",
      "work\tclass\t2.000000\t2.374103\t12\n",
      "represent\tinterest\t3.666667\t2.081666\t3\n",
      "mean\tbourgeois\t4.333333\t2.081666\t3\n",
      "exist\tthing\t4.000000\t1.732051\t3\n",
      "revolutionize\tproduction\t3.333333\t1.154701\t3\n",
      "intend\tproperty\t6.666667\t1.154701\t3\n",
      "concentrate\thand\t3.666667\t1.154701\t3\n",
      "attain\tend\t3.000000\t1.000000\t3\n",
      "exist\tsociety\t1.333333\t0.816497\t6\n",
      "produce\tproduct\t3.500000\t0.577350\t4\n",
      "appropriate\tproduct\t1.600000\t0.547723\t5\n",
      "rule\tclass\t1.000000\t0.000000\t4\n",
      "work\tparty\t2.000000\t0.000000\t4\n",
      "pave\tway\t2.000000\t0.000000\t3\n",
      "put\tend\t2.000000\t0.000000\t3\n",
      "lose\tcharacter\t3.000000\t0.000000\t3\n"
     ]
    }
   ],
   "source": [
    "collocations = distant_collocations(tokens_with_tag, 'V', 'N')\n",
    "\n",
    "for (w1, w2), dev in collocations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode\tproduction\t0.000000\t0.000000\t0\n",
      "socialist\tcommunist\t0.000000\t0.000000\t0\n",
      "division\tlabor\t0.000000\t0.000000\t0\n",
      "community\twoman\t0.000000\t0.000000\t0\n",
      "world\tmarket\t0.000000\t0.000000\t0\n",
      "condition\tlife\t0.000000\t0.000000\t0\n",
      "mean\tsubsistence\t0.000000\t0.000000\t0\n",
      "form\tsociety\t0.000000\t0.000000\t0\n",
      "form\tproperty\t0.000000\t0.000000\t0\n",
      "member\tsociety\t0.000000\t0.000000\t0\n",
      "state\tthing\t0.000000\t0.000000\t0\n",
      "disappearance\tclass\t0.000000\t0.000000\t0\n",
      "benefit\tclass\t0.000000\t0.000000\t0\n",
      "relation\tproduction\t0.000000\t0.000000\t0\n",
      "mean\tcommunication\t0.000000\t0.000000\t0\n",
      "portion\tbourgeoisie\t0.000000\t0.000000\t0\n",
      "section\tclass\t0.000000\t0.000000\t0\n",
      "state\tsociety\t0.000000\t0.000000\t0\n",
      "hand\tstate\t0.000000\t0.000000\t0\n",
      "bourgeois\tsocialism\t0.000000\t0.000000\t1\n"
     ]
    }
   ],
   "source": [
    "collocations = distant_collocations(tokens_with_tag, 'N', 'N')\n",
    "\n",
    "for (w1, w2), dev in collocations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "productive\tforce\t0.000000\t0.000000\t0\n",
      "middle\tage\t0.000000\t0.000000\t0\n",
      "modern\tbourgeois\t0.000000\t0.000000\t0\n",
      "private\tproperty\t0.000000\t0.000000\t0\n",
      "feudal\tsociety\t0.000000\t0.000000\t0\n",
      "middle\tclass\t0.000000\t0.000000\t0\n",
      "petty\tbourgeois\t0.000000\t0.000000\t0\n",
      "absolute\tmonarchy\t0.000000\t0.000000\t0\n",
      "modern\tbourgeoisie\t0.000000\t0.000000\t0\n",
      "free\ttrade\t0.000000\t0.000000\t0\n",
      "bourgeois\tproduction\t0.000000\t0.000000\t1\n",
      "political\tbourgeoisie\t0.000000\t0.000000\t0\n",
      "immense\tmajority\t0.000000\t0.000000\t0\n",
      "french\trevolution\t0.000000\t0.000000\t0\n",
      "mere\tproduction\t0.000000\t0.000000\t0\n",
      "political\tsupremacy\t0.000000\t0.000000\t0\n",
      "eighteenth\tcentury\t0.000000\t0.000000\t0\n",
      "historical\tdevelopment\t0.000000\t0.000000\t0\n",
      "eternal\ttruth\t0.000000\t0.000000\t0\n",
      "undeveloped\tstate\t0.000000\t0.000000\t0\n"
     ]
    }
   ],
   "source": [
    "collocations = distant_collocations(tokens_with_tag, 'J', 'N')\n",
    "\n",
    "for (w1, w2), dev in collocations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\tsocialism\t0.000000\t3.785939\t1\n",
      "communist\tparty\t0.000000\t2.863564\t0\n",
      "communist\tcommunist\t0.000000\t2.645751\t0\n",
      "england\tfrance\t0.000000\t2.516611\t0\n",
      "socialist\tliterature\t0.000000\t2.500000\t0\n",
      "communist\tliterature\t0.000000\t2.500000\t0\n",
      "communism\tpower\t0.000000\t1.527525\t0\n",
      "germany\tbourgeoisie\t0.000000\t0.577350\t0\n",
      "socialist\tcommunist\t0.000000\t0.000000\t0\n"
     ]
    }
   ],
   "source": [
    "collocations = distant_collocations(tokens_with_tag, 'NNP', 'N')\n",
    "\n",
    "for (w1, w2), dev in collocations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implememnt a better lemmatizer for handling proper nouns (NNP / NNPS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word, pos):\n",
    "    if not pos.startswith('NNP'):\n",
    "        word = word.lower()\n",
    "    mapping = {'J': 'a', 'R': 'r', 'N': 'n', 'V': 'v'}\n",
    "    if pos[0] in mapping:\n",
    "        return wordnet_lemmatizer.lemmatize(word, mapping[pos[0]])\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And do not lower() the word in the main function anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distant_collocations(tokens_with_tag, pos1, pos2, min_cut=2, window_size=9):\n",
    "    word_pair_counts = Counter()\n",
    "    word_pair_distance_counts = Counter()\n",
    "    for i in range(len(tokens_with_tag) - 1):\n",
    "        w1, t1 = tokens_with_tag[i]\n",
    "        if not t1.startswith(pos1):\n",
    "            continue\n",
    "        w1 = lemmatize(w1, t1)\n",
    "        for distance in range(1, window_size):\n",
    "            if i + distance < len(tokens_with_tag):\n",
    "                w2, t2 = tokens_with_tag[i + distance]\n",
    "                if t2.startswith(pos2):\n",
    "                    w2 = lemmatize(w2, t2)\n",
    "                    word_pair_distance_counts[(w1, w2, distance)] += 1\n",
    "                    word_pair_counts[(w1, w2)] += 1\n",
    "    \n",
    "    pair_mean_distances = Counter()\n",
    "\n",
    "    for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "        if word_pair_counts[(w1, w2)] > 1:\n",
    "            pair_mean_distances[(w1, w2)] += distance * (c / word_pair_counts[(w1, w2)])\n",
    "\n",
    "    pair_deviations = Counter()\n",
    "    for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "        if w1 in stopword_list:\n",
    "            continue\n",
    "        if word_pair_counts[(w1, w2)] > min_cut:\n",
    "            pair_deviations[(w1, w2)] += c * ((distance - pair_mean_distances[(w1, w2)]) ** 2)\n",
    "    \n",
    "    for (w1, w2), dev_tmp in pair_deviations.most_common():\n",
    "        s_2 = dev_tmp / (word_pair_counts[(w1, w2)] - 1)\n",
    "        pair_deviations[(w1, w2)] = s_2 ** 0.5\n",
    "    \n",
    "    return pair_deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communist\tliterature\t0.000000\t2.886751\t0\n",
      "Socialist\tliterature\t0.000000\t2.886751\t0\n",
      "England\tFrance\t0.000000\t2.516611\t0\n",
      "Communism\tpower\t0.000000\t1.527525\t0\n",
      "Communists\tparty\t0.000000\t1.154701\t0\n",
      "Germany\tbourgeoisie\t0.000000\t0.577350\t0\n",
      "Socialist\tCommunist\t0.000000\t0.000000\t0\n"
     ]
    }
   ],
   "source": [
    "collocations = distant_collocations(tokens_with_tag, 'NNP', 'N')\n",
    "\n",
    "for (w1, w2), dev in collocations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find\twork\t2.666667\t2.886751\t3\n",
      "abolish\tproperty\t5.000000\t2.645751\t3\n",
      "join\tclass\t5.666667\t2.516611\t3\n",
      "fight\tbourgeoisie\t3.800000\t2.387467\t5\n",
      "work\tclass\t2.000000\t2.374103\t12\n",
      "represent\tinterest\t3.666667\t2.081666\t3\n",
      "mean\tbourgeois\t4.333333\t2.081666\t3\n",
      "exist\tthing\t4.000000\t1.732051\t3\n",
      "revolutionize\tproduction\t3.333333\t1.154701\t3\n",
      "intend\tproperty\t6.666667\t1.154701\t3\n",
      "concentrate\thand\t3.666667\t1.154701\t3\n",
      "attain\tend\t3.000000\t1.000000\t3\n",
      "exist\tsociety\t1.333333\t0.816497\t6\n",
      "produce\tproduct\t3.500000\t0.577350\t4\n",
      "appropriate\tproduct\t1.600000\t0.547723\t5\n",
      "rule\tclass\t1.000000\t0.000000\t4\n",
      "work\tparty\t2.000000\t0.000000\t4\n",
      "pave\tway\t2.000000\t0.000000\t3\n",
      "put\tend\t2.000000\t0.000000\t3\n",
      "lose\tcharacter\t3.000000\t0.000000\t3\n"
     ]
    }
   ],
   "source": [
    "collocations = distant_collocations(tokens_with_tag, 'V', 'N')\n",
    "\n",
    "for (w1, w2), dev in collocations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
